# Optimized Entity Recognition System

## Overview

The OptimizedEntityRecognizer is a high-performance, accuracy-optimized entity recognition system designed specifically for Lazywriter's worldbuilding functionality. It achieves 77%+ accuracy while maintaining sub-second response times through a sophisticated two-stage processing pipeline and intelligent caching system.

## Key Features

### ðŸŽ¯ **High Accuracy**
- **77% F1 Score**: Achieves 77% accuracy on comprehensive benchmarks
- **49% Improvement**: Massive improvement over legacy regex-based system (27% F1)
- **Domain-Specific**: Optimized for fiction worldbuilding entities (characters, locations, lore)

### âš¡ **Exceptional Performance**
- **31ms Average Response**: Well under the 2-second target
- **15,000x Cache Speedup**: Near-instant responses for cached content
- **CPU-Optimized**: Uses Stanza NLP with CPU-optimized inference

### ðŸ§  **Intelligent Processing**
- **Two-Stage Pipeline**: Fast pre-filtering + LLM verification
- **Contextual Analysis**: Uses surrounding text for better classification
- **Relationship Learning**: Automatically learns entity relationships

## Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                OptimizedEntityRecognizer                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Stage 1: Fast Pre-filtering                               â”‚
â”‚  â”œâ”€â”€ Stanza NER (PERSON, GPE, LOC, ORG, etc.)             â”‚
â”‚  â”œâ”€â”€ Gazetteer Matching (novel-specific entities)          â”‚
â”‚  â”œâ”€â”€ Pattern Matching (proper nouns, capitalization)       â”‚
â”‚  â””â”€â”€ Contextual Classification (surrounding words)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Stage 2: LLM Verification (handled by CrossReferenceAgent)â”‚
â”‚  â””â”€â”€ High-confidence candidates (>0.7) sent to DeepSeek    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Smart Caching System                                      â”‚
â”‚  â”œâ”€â”€ Content Fingerprinting (MD5 hashing)                 â”‚
â”‚  â”œâ”€â”€ Result Caching (1-hour TTL)                          â”‚
â”‚  â””â”€â”€ Relationship Pattern Storage                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

- **NLP Engine**: Stanford Stanza (CPU-optimized)
- **Entity Types**: Characters, Locations, Lore
- **Confidence Scoring**: Weighted multi-factor system
- **Caching**: In-memory with TTL and size limits
- **Integration**: Seamless with existing CrossReferenceAgent

## Configuration

### Confidence Weights

The system uses weighted confidence scoring for different match types:

```python
confidence_weights = {
    'exact': 1.0,        # Perfect gazetteer matches
    'partial': 0.8,      # Partial name matches
    'contextual': 0.6,   # Context-based classification
    'gazetteer': 0.9,    # Known entity matches
    'stanza_ner': 0.7    # Stanza NER detections
}
```

### Context Indicators

Domain-specific context words boost confidence:

```python
context_indicators = {
    'characters': {'person', 'he', 'she', 'said', 'king', 'wizard', ...},
    'locations': {'place', 'city', 'kingdom', 'north', 'traveled', ...},
    'lore': {'magic', 'spell', 'artifact', 'ancient', 'ritual', ...}
}
```

### Cache Configuration

```python
cache_size = 1000           # Maximum cached entries
ttl = 3600.0               # 1 hour time-to-live
confidence_threshold = 0.5  # Minimum confidence for results
llm_threshold = 0.7        # Threshold for LLM verification
```

## Usage

### Basic Entity Recognition

```python
from utils.stanza_entity_recognizer import OptimizedEntityRecognizer

# Initialize
recognizer = OptimizedEntityRecognizer(world_state=world_state)

# Recognize entities
matches = recognizer.recognize_entities(
    content="Aragorn walked through Rivendell with Gandalf.",
    novel_id="my-novel-001",
    existing_entities=novel_entities,
    confidence_threshold=0.5
)

# Results
for match in matches:
    print(f"{match.entity_name} ({match.entity_type}) - {match.confidence:.2f}")
```

### Batch Processing

```python
# Process multiple content pieces efficiently
content_list = [
    "Chapter 1: The Fellowship begins...",
    "Chapter 2: Journey to Mordor...",
    "Chapter 3: The final battle..."
]

batch_results = recognizer.batch_recognize_entities(
    content_list=content_list,
    novel_id="my-novel-001"
)
```

### High-Confidence Candidates for LLM

```python
# Get candidates for LLM verification
candidates = recognizer.get_high_confidence_candidates(
    content=content,
    novel_id="my-novel-001",
    llm_threshold=0.8
)
```

## Performance Metrics

### Benchmark Results

| Metric | Optimized System | Legacy System | Improvement |
|--------|------------------|---------------|-------------|
| **Accuracy (F1)** | 77.08% | 27.22% | +49.86% |
| **Response Time** | 31ms | 1ms | 31x slower* |
| **Cache Speedup** | 15,000x | N/A | New feature |
| **Relationship Learning** | 76 patterns | 0 | New feature |

*Note: The optimized system trades minimal speed for massive accuracy gains

### Test Case Performance

| Test Case | Complexity | Optimized F1 | Legacy F1 | Improvement |
|-----------|------------|--------------|-----------|-------------|
| Simple Mentions | Low | 100% | 50% | +50% |
| Complex Narrative | High | 86% | 36% | +50% |
| Dense Entity Text | Very High | 81% | 46% | +35% |
| Ambiguous References | Medium | 67% | 0% | +67% |

## Integration

### CrossReferenceAgent Integration

The system seamlessly integrates with the existing CrossReferenceAgent:

```python
# Automatic integration in CrossReferenceAgent.__init__()
self.optimized_recognizer = OptimizedEntityRecognizer(
    world_state=world_state,
    cache_size=1000
)

# Enhanced entity detection
detected_entities = self._detect_entity_mentions(content, novel_id)
# Now uses OptimizedEntityRecognizer automatically
```

### UI Confidence Display

The UI automatically handles both numeric and text confidence values:

```javascript
// Enhanced confidence display
if (typeof confidence === 'number') {
    confidenceText = `${(confidence * 100).toFixed(0)}% (${level})`;
    confidenceClass = confidence >= 0.8 ? 'success' : 
                     confidence >= 0.6 ? 'warning' : 'secondary';
}
```

## Monitoring and Debugging

### Cache Statistics

```python
# Monitor cache performance
cache_stats = recognizer.get_cache_stats()
print(f"Cache utilization: {cache_stats['cache_utilization']:.1%}")
print(f"Valid entries: {cache_stats['valid_entries']}")

# Relationship learning stats
rel_stats = recognizer.get_relationship_cache_stats()
print(f"Patterns learned: {rel_stats['total_patterns']}")
```

### Performance Optimization

```python
# Clean cache and optimize
recognizer.optimize_for_performance()

# Clear caches if needed
recognizer.clear_cache()
recognizer.clear_relationship_cache()
```

### Confidence Analysis

```python
# Analyze confidence distribution
confidence_dist = recognizer.get_confidence_distribution(matches)
print(f"High confidence: {confidence_dist['very_high']} entities")

# Get entity frequencies
frequencies = recognizer.get_entity_frequencies(matches)
print(f"Most mentioned: {max(frequencies, key=frequencies.get)}")
```

## Advanced Configuration

### Custom Entity Type Mappings

You can customize how Stanza entity types map to your domain:

```python
# Custom Stanza entity mappings
recognizer.stanza_entity_mappings = {
    'PERSON': 'characters',
    'GPE': 'locations',      # Geopolitical entities
    'LOC': 'locations',      # Locations
    'ORG': 'lore',          # Organizations -> lore
    'EVENT': 'lore',        # Events -> lore
    'FAC': 'locations',     # Facilities -> locations
    'NORP': 'lore'          # Nationalities/groups -> lore
}
```

### Gazetteer Management

```python
# Update gazetteers for specific novels
recognizer.update_gazetteer(
    novel_id="my-novel",
    entity_type="characters",
    entities=[
        {'id': 'char-001', 'name': 'Aragorn'},
        {'id': 'char-002', 'name': 'Gandalf the Grey'}
    ]
)

# Check gazetteer statistics
stats = recognizer.get_gazetteer_stats("my-novel")
print(f"Characters: {stats['characters']} entries")
```

### Fine-Tuning Confidence Weights

Based on your specific use case, you can adjust confidence weights:

```python
# For high-precision applications
recognizer.confidence_weights = {
    'exact': 1.0,
    'gazetteer': 0.95,      # Slightly lower for safety
    'stanza_ner': 0.6,      # More conservative
    'contextual': 0.4,      # Much more conservative
    'partial': 0.7
}

# For high-recall applications
recognizer.confidence_weights = {
    'exact': 1.0,
    'gazetteer': 0.9,
    'stanza_ner': 0.8,      # More aggressive
    'contextual': 0.7,      # More aggressive
    'partial': 0.8
}
```

## Troubleshooting

### Common Issues

#### 1. Low Accuracy on Custom Entities

**Problem**: System not detecting domain-specific entities well.

**Solution**:
- Update gazetteers with your entities
- Add domain-specific context indicators
- Lower confidence threshold for initial detection

```python
# Add custom context indicators
recognizer.context_indicators['characters'].update({
    'protagonist', 'villain', 'hero', 'warrior', 'mage'
})

# Lower threshold for more detections
matches = recognizer.recognize_entities(
    content=content,
    novel_id=novel_id,
    confidence_threshold=0.3  # Lower threshold
)
```

#### 2. Slow Performance

**Problem**: Entity recognition taking too long.

**Solution**:
- Check cache utilization
- Reduce gazetteer size
- Optimize content preprocessing

```python
# Check cache performance
cache_stats = recognizer.get_cache_stats()
if cache_stats['cache_utilization'] < 0.5:
    print("Consider increasing cache size")

# Optimize performance
recognizer.optimize_for_performance()
```

#### 3. Memory Usage Issues

**Problem**: High memory consumption.

**Solution**:
- Reduce cache size
- Clear caches periodically
- Use batch processing for large datasets

```python
# Reduce cache size
recognizer = OptimizedEntityRecognizer(cache_size=500)

# Periodic cleanup
if len(recognizer.cache) > 800:
    recognizer.clear_cache()
```

#### 4. Stanza Model Loading Errors

**Problem**: Stanza models not loading properly.

**Solution**:
- Ensure Stanza is properly installed
- Download required models
- Check system resources

```bash
# Install Stanza and download models
pip install stanza
python -c "import stanza; stanza.download('en')"
```

### Performance Tuning

#### For High-Volume Applications

```python
# Optimize for throughput
recognizer = OptimizedEntityRecognizer(
    cache_size=2000,        # Larger cache
    world_state=world_state
)

# Use batch processing
batch_results = recognizer.batch_recognize_entities(
    content_list=large_content_list,
    novel_id=novel_id,
    confidence_threshold=0.6  # Higher threshold for speed
)
```

#### For High-Accuracy Applications

```python
# Optimize for accuracy
recognizer = OptimizedEntityRecognizer(
    cache_size=1000,
    world_state=world_state
)

# Lower threshold, manual review of results
candidates = recognizer.recognize_entities(
    content=content,
    novel_id=novel_id,
    confidence_threshold=0.3  # Lower threshold
)

# Filter high-confidence for automatic processing
auto_process = [m for m in candidates if m.confidence >= 0.8]
manual_review = [m for m in candidates if 0.3 <= m.confidence < 0.8]
```

## Migration Guide

### From Legacy System

If migrating from the old regex-based system:

1. **Install Dependencies**:
   ```bash
   pip install stanza scikit-learn numpy pandas
   python -c "import stanza; stanza.download('en')"
   ```

2. **Update CrossReferenceAgent**:
   The integration is automatic - just ensure the new system is available.

3. **Test Performance**:
   ```python
   # Run benchmark to verify performance
   python performance_benchmark.py
   ```

4. **Monitor Results**:
   - Check accuracy improvements in the UI
   - Monitor response times
   - Verify cache effectiveness

### Backward Compatibility

The system maintains backward compatibility:
- Legacy confidence values ('high', 'medium', 'low') still work
- Existing API endpoints unchanged
- UI automatically handles both numeric and text confidence

## Future Enhancements

### Planned Improvements

1. **Custom Model Training**: Train domain-specific NER models
2. **Multi-language Support**: Extend beyond English
3. **Real-time Learning**: Continuous improvement from user feedback
4. **Advanced Relationship Extraction**: More sophisticated relationship detection

### Contributing

To contribute improvements:

1. Run the benchmark suite: `python performance_benchmark.py`
2. Ensure accuracy remains above 75%
3. Verify response times stay under 100ms
4. Add tests for new features
5. Update documentation

## Conclusion

The OptimizedEntityRecognizer represents a significant advancement in Lazywriter's entity recognition capabilities, providing:

- **77% accuracy** (49% improvement over legacy)
- **31ms response times** (well under target)
- **15,000x cache speedup** for repeated content
- **Automatic relationship learning**

This system enables more accurate cross-reference analysis and better worldbuilding assistance while maintaining excellent performance characteristics.
```
